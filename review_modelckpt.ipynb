{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8aba790-326f-4368-a633-00d73ca5227e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting f5_tts\n",
      "  Downloading f5_tts-1.1.7-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (4.52.4)\n",
      "Requirement already satisfied: torch>=2.0.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (2.5.1)\n",
      "Requirement already satisfied: jieba in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (0.42.1)\n",
      "Requirement already satisfied: x_transformers>=1.31.14 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (2.3.23)\n",
      "Requirement already satisfied: pydantic<=2.10.6 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (2.10.1)\n",
      "Requirement already satisfied: transformers_stream_generator in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (0.0.5)\n",
      "Requirement already satisfied: librosa in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (0.11.0)\n",
      "Requirement already satisfied: datasets in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (3.6.0)\n",
      "Requirement already satisfied: gradio<=5.35.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (5.34.2)\n",
      "Requirement already satisfied: torchaudio>=2.0.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (2.5.1)\n",
      "Requirement already satisfied: wandb in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (0.20.1)\n",
      "Requirement already satisfied: pypinyin in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (0.54.0)\n",
      "Requirement already satisfied: bitsandbytes>0.37.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (0.46.0)\n",
      "Requirement already satisfied: pydub in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (0.25.1)\n",
      "Requirement already satisfied: soundfile in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (0.13.1)\n",
      "Requirement already satisfied: safetensors in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (0.5.3)\n",
      "Requirement already satisfied: click in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from f5_tts) (8.0.4)\n",
      "Requirement already satisfied: accelerate>=0.33.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (1.8.1)\n",
      "Requirement already satisfied: vocos in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (0.1.0)\n",
      "Requirement already satisfied: unidecode in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from f5_tts) (1.3.6)\n",
      "Requirement already satisfied: ema_pytorch>=0.5.2 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (0.7.7)\n",
      "Requirement already satisfied: torchdiffeq in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (0.2.5)\n",
      "Requirement already satisfied: matplotlib in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from f5_tts) (3.8.4)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (4.67.1)\n",
      "Requirement already satisfied: cached_path in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (1.7.3)\n",
      "Requirement already satisfied: hydra-core>=1.3.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (1.3.2)\n",
      "Requirement already satisfied: numpy<=1.26.4 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from f5_tts) (1.26.4)\n",
      "Requirement already satisfied: tomli in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from f5_tts) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from accelerate>=0.33.0->f5_tts) (23.0)\n",
      "Requirement already satisfied: psutil in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from accelerate>=0.33.0->f5_tts) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from accelerate>=0.33.0->f5_tts) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from accelerate>=0.33.0->f5_tts) (0.33.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (3.7.1)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (2.1.1)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (0.13.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (4.12.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (0.13.0)\n",
      "Requirement already satisfied: gradio-client==1.10.3 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (1.10.3)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (0.27.2)\n",
      "Requirement already satisfied: ffmpy in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (0.6.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (0.1.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (0.12.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (2.2.3)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (0.1.6)\n",
      "Requirement already satisfied: orjson~=3.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (3.10.18)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (0.34.3)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (0.46.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (3.1.2)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (24.1.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (0.0.20)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (0.115.13)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from gradio<=5.35.0->f5_tts) (9.4.0)\n",
      "Requirement already satisfied: fsspec in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio-client==1.10.3->gradio<=5.35.0->f5_tts) (2024.10.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gradio-client==1.10.3->gradio<=5.35.0->f5_tts) (10.4)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from hydra-core>=1.3.0->f5_tts) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from hydra-core>=1.3.0->f5_tts) (4.9.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from pydantic<=2.10.6->f5_tts) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from pydantic<=2.10.6->f5_tts) (2.27.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (3.1.0)\n",
      "Requirement already satisfied: networkx in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (3.4.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (2.21.5)\n",
      "Requirement already satisfied: filelock in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=2.0.0->f5_tts) (12.4.5.8)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->f5_tts) (1.3.0)\n",
      "Requirement already satisfied: loguru in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from x_transformers>=1.31.14->f5_tts) (0.7.3)\n",
      "Requirement already satisfied: einops>=0.8.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from x_transformers>=1.31.14->f5_tts) (0.8.1)\n",
      "Requirement already satisfied: einx>=0.3.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from x_transformers>=1.31.14->f5_tts) (0.3.0)\n",
      "Requirement already satisfied: rich<14.0,>=12.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from cached_path->f5_tts) (13.9.4)\n",
      "Requirement already satisfied: boto3<2.0,>=1.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from cached_path->f5_tts) (1.38.41)\n",
      "Requirement already satisfied: requests in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from cached_path->f5_tts) (2.32.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from cached_path->f5_tts) (2.19.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from datasets->f5_tts) (0.70.16)\n",
      "Requirement already satisfied: xxhash in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from datasets->f5_tts) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from datasets->f5_tts) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from datasets->f5_tts) (0.3.8)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from librosa->f5_tts) (0.4)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from librosa->f5_tts) (3.0.1)\n",
      "Requirement already satisfied: msgpack>=1.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from librosa->f5_tts) (1.0.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from librosa->f5_tts) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from librosa->f5_tts) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from librosa->f5_tts) (5.1.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from librosa->f5_tts) (1.2.2)\n",
      "Requirement already satisfied: pooch>=1.1 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from librosa->f5_tts) (1.4.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from librosa->f5_tts) (0.61.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from librosa->f5_tts) (0.5.0.post1)\n",
      "Requirement already satisfied: cffi>=1.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from soundfile->f5_tts) (1.15.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from matplotlib->f5_tts) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from matplotlib->f5_tts) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from matplotlib->f5_tts) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from matplotlib->f5_tts) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from matplotlib->f5_tts) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from matplotlib->f5_tts) (0.12.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from transformers->f5_tts) (0.21.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from transformers->f5_tts) (2024.11.6)\n",
      "Requirement already satisfied: encodec==0.1.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from vocos->f5_tts) (0.1.1)\n",
      "Requirement already satisfied: platformdirs in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from wandb->f5_tts) (4.3.6)\n",
      "Requirement already satisfied: setproctitle in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from wandb->f5_tts) (1.3.6)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from wandb->f5_tts) (3.1.44)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from wandb->f5_tts) (2.30.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from wandb->f5_tts) (5.28.3)\n",
      "Requirement already satisfied: idna>=2.8 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.35.0->f5_tts) (3.4)\n",
      "Requirement already satisfied: exceptiongroup in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.35.0->f5_tts) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.35.0->f5_tts) (1.3.1)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.41 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from boto3<2.0,>=1.0->cached_path->f5_tts) (1.38.41)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from boto3<2.0,>=1.0->cached_path->f5_tts) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from boto3<2.0,>=1.0->cached_path->f5_tts) (0.13.0)\n",
      "Requirement already satisfied: pycparser in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from cffi>=1.0->soundfile->f5_tts) (2.21)\n",
      "Requirement already satisfied: frozendict in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from einx>=0.3.0->x_transformers>=1.31.14->f5_tts) (2.4.6)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from fsspec->gradio-client==1.10.3->gradio<=5.35.0->f5_tts) (3.11.7)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->f5_tts) (4.0.12)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts) (2.36.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts) (2.23.0)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts) (2.7.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts) (2.4.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts) (1.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<=5.35.0->f5_tts) (1.0.6)\n",
      "Requirement already satisfied: certifi in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<=5.35.0->f5_tts) (2024.6.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio<=5.35.0->f5_tts) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.33.0->f5_tts) (1.1.5)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from numba>=0.51.0->librosa->f5_tts) (0.44.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio<=5.35.0->f5_tts) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio<=5.35.0->f5_tts) (2024.1)\n",
      "Requirement already satisfied: appdirs in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from pooch>=1.1->librosa->f5_tts) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->f5_tts) (1.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from requests->cached_path->f5_tts) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from requests->cached_path->f5_tts) (2.0.4)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from rich<14.0,>=12.1->cached_path->f5_tts) (2.15.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from rich<14.0,>=12.1->cached_path->f5_tts) (3.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa->f5_tts) (2.2.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<=5.35.0->f5_tts) (1.5.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==1.10.3->gradio<=5.35.0->f5_tts) (6.1.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==1.10.3->gradio<=5.35.0->f5_tts) (5.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==1.10.3->gradio<=5.35.0->f5_tts) (1.18.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==1.10.3->gradio<=5.35.0->f5_tts) (2.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==1.10.3->gradio<=5.35.0->f5_tts) (1.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==1.10.3->gradio<=5.35.0->f5_tts) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==1.10.3->gradio<=5.35.0->f5_tts) (1.3.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==1.10.3->gradio<=5.35.0->f5_tts) (0.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->f5_tts) (5.0.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts) (1.25.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts) (1.66.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts) (5.5.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts) (0.4.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0,>=12.1->cached_path->f5_tts) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts) (0.6.1)\n",
      "Installing collected packages: f5_tts\n",
      "Successfully installed f5_tts-1.1.7\n"
     ]
    }
   ],
   "source": [
    "!pip install f5_tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73649bc3-226d-409f-a988-5a9dff671947",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting peft\n",
      "  Downloading peft-0.16.0-py3-none-any.whl (472 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.3/472.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: pyyaml in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from peft) (1.8.1)\n",
      "Requirement already satisfied: safetensors in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: psutil in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from peft) (0.33.0)\n",
      "Requirement already satisfied: transformers in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from peft) (4.52.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from peft) (23.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (2024.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
      "Requirement already satisfied: filelock in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: requests in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: jinja2 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: networkx in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /nas/longleaf/home/rphadke/.local/lib/python3.10/site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2024.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.0.4)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f56744-0917-450a-b28a-b29e8c48b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from safetensors.torch import load_file\n",
    "from f5_tts.model.utils import get_tokenizer\n",
    "import warnings; warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "946e9076-3475-4376-aab4-43f19ba952e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_lora_weights(sd):\n",
    "    # find all LoRA params in the after-finetuned checkpoint\n",
    "    lora_items = {k: v for k,v in sd.items() if \".lora_\" in k and isinstance(v, torch.Tensor)}\n",
    "    if not lora_items:\n",
    "        print(\"No LoRA weights found in this checkpoint.\")\n",
    "        return\n",
    "\n",
    "    print(\"LoRA adapter parameter stats:\\n\")\n",
    "    total_norm2 = 0.0\n",
    "    total_elements = 0\n",
    "    for name, tensor in sorted(lora_items.items()):\n",
    "        t = tensor.cpu()\n",
    "        mean_abs = t.abs().mean().item()\n",
    "        max_abs  = t.abs().max().item()\n",
    "        norm2    = t.norm().item()\n",
    "        total_norm2 += norm2**2\n",
    "        total_elements += t.numel()\n",
    "        print(f\"{name:60s}  shape={tuple(t.shape)}  mean|·|={mean_abs:.3e} max|·|={max_abs:.3e} ‖·‖₂={norm2:.3e}\")\n",
    "\n",
    "    overall_fro = total_norm2**0.5\n",
    "    avg_norm    = overall_fro / (total_elements**0.5)\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"  Total LoRA parameters : {total_elements:,}\")\n",
    "    print(f\"  Combined Frobenius ‖Δ‖₂ : {overall_fro:.3e}\")\n",
    "    print(f\"  Avg per-param scale    : {avg_norm:.3e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b46d14-d934-4ecf-a365-691e784e6a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " == Prior Checkpoint ==\n",
      "\n",
      " == New Checkpoint ==\n"
     ]
    }
   ],
   "source": [
    "# Path to your checkpoints\n",
    "before_ckpt = \"/work/users/r/p/rphadke/JSALT/ckpts/fisher_chunks_1K_LoRAv3.4/model_1000.pt\"\n",
    "after_ckpt = \"/work/users/r/p/rphadke/JSALT/ckpts/fisher_chunks_1K_LoRAv3.4/model_9000.pt\"\n",
    "tok_path = \"/work/users/r/p/rphadke/JSALT/fisher_chunks_0.1K_v2.1/vocab.txt\"\n",
    "vocab_char_map, vocab_size = get_tokenizer(tok_path, \"custom\")\n",
    "\n",
    "# ID of your new speaker token (e.g. tokenizer.convert_tokens_to_ids(\"<spk>\"))\n",
    "spk_chg_token = \"-\"\n",
    "\n",
    "    # 1) Load state dicts\n",
    "sd_before = torch.load(before_ckpt,  map_location=\"cpu\")[\"model_state_dict\"]\n",
    "print(\"\\n == Prior Checkpoint ==\")\n",
    "# print(set(sd_before.keys()))\n",
    "sd_after  = torch.load(after_ckpt,  map_location=\"cpu\")[\"model_state_dict\"]\n",
    "sd_after = {k.replace(\"ema_model.\", \"\"): v for k, v in sd_after.items()}\n",
    "print(\"\\n == New Checkpoint ==\")\n",
    "# print(set(sd_after.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33a484be-6223-42e3-a1fa-3f74c6574f8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Text Embedding Drift ==\n",
      "→ Max text embedding matrix change |Δ| overall:                  0.000003\n",
      "→ Max |Δ| on token tensor([[2545]]): 0.000000\n",
      "\n",
      "== Input Embedding Drift ==\n",
      "→ Mean input embedding weight change |Δ| overall:                  0.000051\n",
      "== LoRA Weight Scales ==\n",
      "LoRA adapter parameter stats:\n",
      "\n",
      "base_model.model.transformer.text_embed.text_blocks.0.pwconv1.lora_A.default.weight  shape=(64, 512)  mean|·|=2.207e-02 max|·|=4.419e-02 ‖·‖₂=4.615e+00\n",
      "base_model.model.transformer.text_embed.text_blocks.0.pwconv1.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.530e-07 max|·|=4.475e-06 ‖·‖₂=1.568e-04\n",
      "base_model.model.transformer.text_embed.text_blocks.0.pwconv2.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.562e-02 max|·|=3.125e-02 ‖·‖₂=4.617e+00\n",
      "base_model.model.transformer.text_embed.text_blocks.0.pwconv2.lora_B.default.weight  shape=(512, 64)  mean|·|=6.030e-07 max|·|=5.283e-06 ‖·‖₂=1.454e-04\n",
      "base_model.model.transformer.text_embed.text_blocks.1.pwconv1.lora_A.default.weight  shape=(64, 512)  mean|·|=2.226e-02 max|·|=4.419e-02 ‖·‖₂=4.644e+00\n",
      "base_model.model.transformer.text_embed.text_blocks.1.pwconv1.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.143e-07 max|·|=3.907e-06 ‖·‖₂=1.410e-04\n",
      "base_model.model.transformer.text_embed.text_blocks.1.pwconv2.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.564e-02 max|·|=3.125e-02 ‖·‖₂=4.623e+00\n",
      "base_model.model.transformer.text_embed.text_blocks.1.pwconv2.lora_B.default.weight  shape=(512, 64)  mean|·|=5.172e-07 max|·|=5.029e-06 ‖·‖₂=1.261e-04\n",
      "base_model.model.transformer.text_embed.text_blocks.2.pwconv1.lora_A.default.weight  shape=(64, 512)  mean|·|=2.205e-02 max|·|=4.419e-02 ‖·‖₂=4.612e+00\n",
      "base_model.model.transformer.text_embed.text_blocks.2.pwconv1.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.853e-07 max|·|=3.994e-06 ‖·‖₂=1.324e-04\n",
      "base_model.model.transformer.text_embed.text_blocks.2.pwconv2.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.564e-02 max|·|=3.125e-02 ‖·‖₂=4.625e+00\n",
      "base_model.model.transformer.text_embed.text_blocks.2.pwconv2.lora_B.default.weight  shape=(512, 64)  mean|·|=4.641e-07 max|·|=3.516e-06 ‖·‖₂=1.103e-04\n",
      "base_model.model.transformer.text_embed.text_blocks.3.pwconv1.lora_A.default.weight  shape=(64, 512)  mean|·|=2.206e-02 max|·|=4.419e-02 ‖·‖₂=4.614e+00\n",
      "base_model.model.transformer.text_embed.text_blocks.3.pwconv1.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.134e-07 max|·|=5.885e-06 ‖·‖₂=1.874e-04\n",
      "base_model.model.transformer.text_embed.text_blocks.3.pwconv2.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.125e-02 ‖·‖₂=4.620e+00\n",
      "base_model.model.transformer.text_embed.text_blocks.3.pwconv2.lora_B.default.weight  shape=(512, 64)  mean|·|=9.250e-07 max|·|=7.885e-06 ‖·‖₂=2.272e-04\n",
      "base_model.model.transformer.transformer_blocks.0.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.568e-02 max|·|=3.125e-02 ‖·‖₂=4.634e+00\n",
      "base_model.model.transformer.transformer_blocks.0.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=9.654e-07 max|·|=1.113e-05 ‖·‖₂=3.552e-04\n",
      "base_model.model.transformer.transformer_blocks.0.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.563e-02 max|·|=3.126e-02 ‖·‖₂=4.617e+00\n",
      "base_model.model.transformer.transformer_blocks.0.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.707e-06 max|·|=5.997e-05 ‖·‖₂=2.144e-03\n",
      "base_model.model.transformer.transformer_blocks.0.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.125e-02 ‖·‖₂=4.615e+00\n",
      "base_model.model.transformer.transformer_blocks.0.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=8.916e-07 max|·|=9.310e-06 ‖·‖₂=3.537e-04\n",
      "base_model.model.transformer.transformer_blocks.0.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.562e-02 max|·|=3.128e-02 ‖·‖₂=4.616e+00\n",
      "base_model.model.transformer.transformer_blocks.0.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.909e-06 max|·|=4.372e-05 ‖·‖₂=1.983e-03\n",
      "base_model.model.transformer.transformer_blocks.0.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.557e-02 max|·|=3.127e-02 ‖·‖₂=4.607e+00\n",
      "base_model.model.transformer.transformer_blocks.0.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=7.410e-06 max|·|=5.106e-05 ‖·‖₂=3.470e-03\n",
      "base_model.model.transformer.transformer_blocks.0.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.105e-02 max|·|=2.212e-02 ‖·‖₂=4.624e+00\n",
      "base_model.model.transformer.transformer_blocks.0.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.872e-06 max|·|=5.455e-04 ‖·‖₂=3.428e-03\n",
      "base_model.model.transformer.transformer_blocks.1.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.564e-02 max|·|=3.125e-02 ‖·‖₂=4.626e+00\n",
      "base_model.model.transformer.transformer_blocks.1.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=1.320e-06 max|·|=1.658e-05 ‖·‖₂=4.898e-04\n",
      "base_model.model.transformer.transformer_blocks.1.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.565e-02 max|·|=3.127e-02 ‖·‖₂=4.623e+00\n",
      "base_model.model.transformer.transformer_blocks.1.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.240e-06 max|·|=5.063e-05 ‖·‖₂=1.790e-03\n",
      "base_model.model.transformer.transformer_blocks.1.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.563e-02 max|·|=3.125e-02 ‖·‖₂=4.619e+00\n",
      "base_model.model.transformer.transformer_blocks.1.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=1.372e-06 max|·|=2.114e-05 ‖·‖₂=5.258e-04\n",
      "base_model.model.transformer.transformer_blocks.1.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.560e-02 max|·|=3.127e-02 ‖·‖₂=4.613e+00\n",
      "base_model.model.transformer.transformer_blocks.1.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.941e-06 max|·|=2.660e-05 ‖·‖₂=1.889e-03\n",
      "base_model.model.transformer.transformer_blocks.1.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.564e-02 max|·|=3.127e-02 ‖·‖₂=4.621e+00\n",
      "base_model.model.transformer.transformer_blocks.1.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=4.841e-06 max|·|=4.173e-05 ‖·‖₂=2.312e-03\n",
      "base_model.model.transformer.transformer_blocks.1.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.107e-02 max|·|=2.211e-02 ‖·‖₂=4.627e+00\n",
      "base_model.model.transformer.transformer_blocks.1.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.197e-06 max|·|=5.612e-05 ‖·‖₂=2.068e-03\n",
      "base_model.model.transformer.transformer_blocks.10.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.565e-02 max|·|=3.126e-02 ‖·‖₂=4.621e+00\n",
      "base_model.model.transformer.transformer_blocks.10.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.993e-06 max|·|=2.595e-05 ‖·‖₂=1.377e-03\n",
      "base_model.model.transformer.transformer_blocks.10.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.563e-02 max|·|=3.127e-02 ‖·‖₂=4.618e+00\n",
      "base_model.model.transformer.transformer_blocks.10.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.864e-06 max|·|=2.862e-05 ‖·‖₂=2.161e-03\n",
      "base_model.model.transformer.transformer_blocks.10.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.568e-02 max|·|=3.127e-02 ‖·‖₂=4.630e+00\n",
      "base_model.model.transformer.transformer_blocks.10.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.047e-06 max|·|=2.840e-05 ‖·‖₂=1.432e-03\n",
      "base_model.model.transformer.transformer_blocks.10.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.127e-02 ‖·‖₂=4.611e+00\n",
      "base_model.model.transformer.transformer_blocks.10.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=7.986e-06 max|·|=3.379e-05 ‖·‖₂=2.420e-03\n",
      "base_model.model.transformer.transformer_blocks.10.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.560e-02 max|·|=3.127e-02 ‖·‖₂=4.613e+00\n",
      "base_model.model.transformer.transformer_blocks.10.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=5.491e-06 max|·|=5.137e-05 ‖·‖₂=2.668e-03\n",
      "base_model.model.transformer.transformer_blocks.10.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.102e-02 max|·|=2.211e-02 ‖·‖₂=4.609e+00\n",
      "base_model.model.transformer.transformer_blocks.10.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.916e-06 max|·|=3.735e-05 ‖·‖₂=1.906e-03\n",
      "base_model.model.transformer.transformer_blocks.11.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.566e-02 max|·|=3.126e-02 ‖·‖₂=4.625e+00\n",
      "base_model.model.transformer.transformer_blocks.11.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.279e-06 max|·|=4.280e-05 ‖·‖₂=1.473e-03\n",
      "base_model.model.transformer.transformer_blocks.11.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.558e-02 max|·|=3.127e-02 ‖·‖₂=4.610e+00\n",
      "base_model.model.transformer.transformer_blocks.11.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.572e-06 max|·|=3.578e-05 ‖·‖₂=2.072e-03\n",
      "base_model.model.transformer.transformer_blocks.11.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.565e-02 max|·|=3.126e-02 ‖·‖₂=4.624e+00\n",
      "base_model.model.transformer.transformer_blocks.11.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.668e-06 max|·|=2.664e-05 ‖·‖₂=1.263e-03\n",
      "base_model.model.transformer.transformer_blocks.11.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.562e-02 max|·|=3.127e-02 ‖·‖₂=4.621e+00\n",
      "base_model.model.transformer.transformer_blocks.11.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=7.999e-06 max|·|=2.664e-05 ‖·‖₂=2.398e-03\n",
      "base_model.model.transformer.transformer_blocks.11.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.556e-02 max|·|=3.127e-02 ‖·‖₂=4.607e+00\n",
      "base_model.model.transformer.transformer_blocks.11.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=6.113e-06 max|·|=5.111e-05 ‖·‖₂=2.889e-03\n",
      "base_model.model.transformer.transformer_blocks.11.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.107e-02 max|·|=2.212e-02 ‖·‖₂=4.625e+00\n",
      "base_model.model.transformer.transformer_blocks.11.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.197e-06 max|·|=3.590e-05 ‖·‖₂=1.990e-03\n",
      "base_model.model.transformer.transformer_blocks.12.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.559e-02 max|·|=3.126e-02 ‖·‖₂=4.613e+00\n",
      "base_model.model.transformer.transformer_blocks.12.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.306e-06 max|·|=3.258e-05 ‖·‖₂=1.179e-03\n",
      "base_model.model.transformer.transformer_blocks.12.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.127e-02 ‖·‖₂=4.615e+00\n",
      "base_model.model.transformer.transformer_blocks.12.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.852e-06 max|·|=2.908e-05 ‖·‖₂=1.911e-03\n",
      "base_model.model.transformer.transformer_blocks.12.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.560e-02 max|·|=3.126e-02 ‖·‖₂=4.613e+00\n",
      "base_model.model.transformer.transformer_blocks.12.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=2.975e-06 max|·|=2.819e-05 ‖·‖₂=1.076e-03\n",
      "base_model.model.transformer.transformer_blocks.12.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.559e-02 max|·|=3.127e-02 ‖·‖₂=4.616e+00\n",
      "base_model.model.transformer.transformer_blocks.12.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=7.359e-06 max|·|=3.345e-05 ‖·‖₂=2.326e-03\n",
      "base_model.model.transformer.transformer_blocks.12.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.565e-02 max|·|=3.127e-02 ‖·‖₂=4.627e+00\n",
      "base_model.model.transformer.transformer_blocks.12.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=5.059e-06 max|·|=5.133e-05 ‖·‖₂=2.462e-03\n",
      "base_model.model.transformer.transformer_blocks.12.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.103e-02 max|·|=2.211e-02 ‖·‖₂=4.613e+00\n",
      "base_model.model.transformer.transformer_blocks.12.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.881e-06 max|·|=3.207e-05 ‖·‖₂=1.601e-03\n",
      "base_model.model.transformer.transformer_blocks.13.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.562e-02 max|·|=3.125e-02 ‖·‖₂=4.616e+00\n",
      "base_model.model.transformer.transformer_blocks.13.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.482e-06 max|·|=2.942e-05 ‖·‖₂=1.195e-03\n",
      "base_model.model.transformer.transformer_blocks.13.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.564e-02 max|·|=3.127e-02 ‖·‖₂=4.623e+00\n",
      "base_model.model.transformer.transformer_blocks.13.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.619e-06 max|·|=3.670e-05 ‖·‖₂=1.834e-03\n",
      "base_model.model.transformer.transformer_blocks.13.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.563e-02 max|·|=3.125e-02 ‖·‖₂=4.619e+00\n",
      "base_model.model.transformer.transformer_blocks.13.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.221e-06 max|·|=2.442e-05 ‖·‖₂=1.116e-03\n",
      "base_model.model.transformer.transformer_blocks.13.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.560e-02 max|·|=3.127e-02 ‖·‖₂=4.615e+00\n",
      "base_model.model.transformer.transformer_blocks.13.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=7.006e-06 max|·|=2.816e-05 ‖·‖₂=2.155e-03\n",
      "base_model.model.transformer.transformer_blocks.13.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.557e-02 max|·|=3.126e-02 ‖·‖₂=4.606e+00\n",
      "base_model.model.transformer.transformer_blocks.13.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=4.874e-06 max|·|=3.738e-05 ‖·‖₂=2.343e-03\n",
      "base_model.model.transformer.transformer_blocks.13.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.105e-02 max|·|=2.212e-02 ‖·‖₂=4.618e+00\n",
      "base_model.model.transformer.transformer_blocks.13.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.928e-06 max|·|=4.897e-05 ‖·‖₂=1.947e-03\n",
      "base_model.model.transformer.transformer_blocks.14.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.559e-02 max|·|=3.126e-02 ‖·‖₂=4.614e+00\n",
      "base_model.model.transformer.transformer_blocks.14.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.287e-06 max|·|=2.438e-05 ‖·‖₂=1.144e-03\n",
      "base_model.model.transformer.transformer_blocks.14.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.565e-02 max|·|=3.127e-02 ‖·‖₂=4.626e+00\n",
      "base_model.model.transformer.transformer_blocks.14.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.146e-06 max|·|=3.420e-05 ‖·‖₂=1.970e-03\n",
      "base_model.model.transformer.transformer_blocks.14.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.126e-02 ‖·‖₂=4.617e+00\n",
      "base_model.model.transformer.transformer_blocks.14.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.078e-06 max|·|=2.499e-05 ‖·‖₂=1.082e-03\n",
      "base_model.model.transformer.transformer_blocks.14.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.126e-02 ‖·‖₂=4.615e+00\n",
      "base_model.model.transformer.transformer_blocks.14.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.195e-06 max|·|=3.648e-05 ‖·‖₂=1.974e-03\n",
      "base_model.model.transformer.transformer_blocks.14.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.560e-02 max|·|=3.127e-02 ‖·‖₂=4.616e+00\n",
      "base_model.model.transformer.transformer_blocks.14.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=5.106e-06 max|·|=3.666e-05 ‖·‖₂=2.443e-03\n",
      "base_model.model.transformer.transformer_blocks.14.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.105e-02 max|·|=2.212e-02 ‖·‖₂=4.620e+00\n",
      "base_model.model.transformer.transformer_blocks.14.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.991e-06 max|·|=3.253e-05 ‖·‖₂=1.890e-03\n",
      "base_model.model.transformer.transformer_blocks.15.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.560e-02 max|·|=3.125e-02 ‖·‖₂=4.611e+00\n",
      "base_model.model.transformer.transformer_blocks.15.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.520e-06 max|·|=2.765e-05 ‖·‖₂=1.484e-03\n",
      "base_model.model.transformer.transformer_blocks.15.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.567e-02 max|·|=3.127e-02 ‖·‖₂=4.629e+00\n",
      "base_model.model.transformer.transformer_blocks.15.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.850e-06 max|·|=2.995e-05 ‖·‖₂=2.143e-03\n",
      "base_model.model.transformer.transformer_blocks.15.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.567e-02 max|·|=3.126e-02 ‖·‖₂=4.626e+00\n",
      "base_model.model.transformer.transformer_blocks.15.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.588e-06 max|·|=2.546e-05 ‖·‖₂=1.505e-03\n",
      "base_model.model.transformer.transformer_blocks.15.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.564e-02 max|·|=3.127e-02 ‖·‖₂=4.623e+00\n",
      "base_model.model.transformer.transformer_blocks.15.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=7.299e-06 max|·|=3.077e-05 ‖·‖₂=2.254e-03\n",
      "base_model.model.transformer.transformer_blocks.15.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.570e-02 max|·|=3.127e-02 ‖·‖₂=4.633e+00\n",
      "base_model.model.transformer.transformer_blocks.15.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=5.916e-06 max|·|=4.265e-05 ‖·‖₂=2.734e-03\n",
      "base_model.model.transformer.transformer_blocks.15.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.106e-02 max|·|=2.212e-02 ‖·‖₂=4.625e+00\n",
      "base_model.model.transformer.transformer_blocks.15.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.911e-06 max|·|=4.507e-05 ‖·‖₂=2.159e-03\n",
      "base_model.model.transformer.transformer_blocks.16.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.126e-02 ‖·‖₂=4.614e+00\n",
      "base_model.model.transformer.transformer_blocks.16.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.126e-06 max|·|=3.345e-05 ‖·‖₂=1.956e-03\n",
      "base_model.model.transformer.transformer_blocks.16.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.559e-02 max|·|=3.127e-02 ‖·‖₂=4.616e+00\n",
      "base_model.model.transformer.transformer_blocks.16.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=8.290e-06 max|·|=3.178e-05 ‖·‖₂=2.522e-03\n",
      "base_model.model.transformer.transformer_blocks.16.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.568e-02 max|·|=3.127e-02 ‖·‖₂=4.631e+00\n",
      "base_model.model.transformer.transformer_blocks.16.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.950e-06 max|·|=4.180e-05 ‖·‖₂=1.913e-03\n",
      "base_model.model.transformer.transformer_blocks.16.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.562e-02 max|·|=3.126e-02 ‖·‖₂=4.615e+00\n",
      "base_model.model.transformer.transformer_blocks.16.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=7.746e-06 max|·|=2.694e-05 ‖·‖₂=2.314e-03\n",
      "base_model.model.transformer.transformer_blocks.16.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.553e-02 max|·|=3.127e-02 ‖·‖₂=4.599e+00\n",
      "base_model.model.transformer.transformer_blocks.16.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=6.108e-06 max|·|=5.369e-05 ‖·‖₂=2.829e-03\n",
      "base_model.model.transformer.transformer_blocks.16.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.104e-02 max|·|=2.211e-02 ‖·‖₂=4.616e+00\n",
      "base_model.model.transformer.transformer_blocks.16.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=7.054e-06 max|·|=3.770e-05 ‖·‖₂=2.210e-03\n",
      "base_model.model.transformer.transformer_blocks.17.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.555e-02 max|·|=3.126e-02 ‖·‖₂=4.604e+00\n",
      "base_model.model.transformer.transformer_blocks.17.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.145e-06 max|·|=2.605e-05 ‖·‖₂=1.363e-03\n",
      "base_model.model.transformer.transformer_blocks.17.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.566e-02 max|·|=3.127e-02 ‖·‖₂=4.623e+00\n",
      "base_model.model.transformer.transformer_blocks.17.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.785e-06 max|·|=4.632e-05 ‖·‖₂=2.152e-03\n",
      "base_model.model.transformer.transformer_blocks.17.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.560e-02 max|·|=3.127e-02 ‖·‖₂=4.616e+00\n",
      "base_model.model.transformer.transformer_blocks.17.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.953e-06 max|·|=2.316e-05 ‖·‖₂=1.293e-03\n",
      "base_model.model.transformer.transformer_blocks.17.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.566e-02 max|·|=3.127e-02 ‖·‖₂=4.627e+00\n",
      "base_model.model.transformer.transformer_blocks.17.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=8.064e-06 max|·|=3.366e-05 ‖·‖₂=2.481e-03\n",
      "base_model.model.transformer.transformer_blocks.17.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.569e-02 max|·|=3.127e-02 ‖·‖₂=4.633e+00\n",
      "base_model.model.transformer.transformer_blocks.17.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=5.684e-06 max|·|=4.226e-05 ‖·‖₂=2.695e-03\n",
      "base_model.model.transformer.transformer_blocks.17.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.106e-02 max|·|=2.211e-02 ‖·‖₂=4.622e+00\n",
      "base_model.model.transformer.transformer_blocks.17.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.444e-06 max|·|=3.445e-05 ‖·‖₂=2.013e-03\n",
      "base_model.model.transformer.transformer_blocks.18.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.556e-02 max|·|=3.126e-02 ‖·‖₂=4.603e+00\n",
      "base_model.model.transformer.transformer_blocks.18.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.186e-06 max|·|=3.062e-05 ‖·‖₂=1.423e-03\n",
      "base_model.model.transformer.transformer_blocks.18.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.557e-02 max|·|=3.127e-02 ‖·‖₂=4.607e+00\n",
      "base_model.model.transformer.transformer_blocks.18.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.703e-06 max|·|=3.711e-05 ‖·‖₂=2.124e-03\n",
      "base_model.model.transformer.transformer_blocks.18.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.560e-02 max|·|=3.126e-02 ‖·‖₂=4.616e+00\n",
      "base_model.model.transformer.transformer_blocks.18.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.146e-06 max|·|=3.550e-05 ‖·‖₂=1.405e-03\n",
      "base_model.model.transformer.transformer_blocks.18.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.567e-02 max|·|=3.127e-02 ‖·‖₂=4.632e+00\n",
      "base_model.model.transformer.transformer_blocks.18.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.158e-06 max|·|=3.613e-05 ‖·‖₂=1.931e-03\n",
      "base_model.model.transformer.transformer_blocks.18.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.563e-02 max|·|=3.127e-02 ‖·‖₂=4.619e+00\n",
      "base_model.model.transformer.transformer_blocks.18.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=4.949e-06 max|·|=3.943e-05 ‖·‖₂=2.387e-03\n",
      "base_model.model.transformer.transformer_blocks.18.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.105e-02 max|·|=2.212e-02 ‖·‖₂=4.618e+00\n",
      "base_model.model.transformer.transformer_blocks.18.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.422e-06 max|·|=5.371e-05 ‖·‖₂=2.080e-03\n",
      "base_model.model.transformer.transformer_blocks.19.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.566e-02 max|·|=3.125e-02 ‖·‖₂=4.628e+00\n",
      "base_model.model.transformer.transformer_blocks.19.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.038e-06 max|·|=2.373e-05 ‖·‖₂=1.303e-03\n",
      "base_model.model.transformer.transformer_blocks.19.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.562e-02 max|·|=3.126e-02 ‖·‖₂=4.616e+00\n",
      "base_model.model.transformer.transformer_blocks.19.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.564e-06 max|·|=3.364e-05 ‖·‖₂=1.813e-03\n",
      "base_model.model.transformer.transformer_blocks.19.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.557e-02 max|·|=3.126e-02 ‖·‖₂=4.609e+00\n",
      "base_model.model.transformer.transformer_blocks.19.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.038e-06 max|·|=2.559e-05 ‖·‖₂=1.313e-03\n",
      "base_model.model.transformer.transformer_blocks.19.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.564e-02 max|·|=3.127e-02 ‖·‖₂=4.622e+00\n",
      "base_model.model.transformer.transformer_blocks.19.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.384e-06 max|·|=3.035e-05 ‖·‖₂=2.020e-03\n",
      "base_model.model.transformer.transformer_blocks.19.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.554e-02 max|·|=3.127e-02 ‖·‖₂=4.597e+00\n",
      "base_model.model.transformer.transformer_blocks.19.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=5.514e-06 max|·|=1.409e-04 ‖·‖₂=2.610e-03\n",
      "base_model.model.transformer.transformer_blocks.19.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.101e-02 max|·|=2.212e-02 ‖·‖₂=4.609e+00\n",
      "base_model.model.transformer.transformer_blocks.19.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.905e-06 max|·|=4.010e-05 ‖·‖₂=2.213e-03\n",
      "base_model.model.transformer.transformer_blocks.2.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.565e-02 max|·|=3.125e-02 ‖·‖₂=4.623e+00\n",
      "base_model.model.transformer.transformer_blocks.2.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=2.167e-06 max|·|=2.719e-05 ‖·‖₂=8.750e-04\n",
      "base_model.model.transformer.transformer_blocks.2.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.562e-02 max|·|=3.127e-02 ‖·‖₂=4.617e+00\n",
      "base_model.model.transformer.transformer_blocks.2.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.435e-06 max|·|=5.583e-05 ‖·‖₂=2.096e-03\n",
      "base_model.model.transformer.transformer_blocks.2.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.558e-02 max|·|=3.126e-02 ‖·‖₂=4.610e+00\n",
      "base_model.model.transformer.transformer_blocks.2.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=1.993e-06 max|·|=2.808e-05 ‖·‖₂=8.316e-04\n",
      "base_model.model.transformer.transformer_blocks.2.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.559e-02 max|·|=3.127e-02 ‖·‖₂=4.610e+00\n",
      "base_model.model.transformer.transformer_blocks.2.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=7.503e-06 max|·|=2.799e-05 ‖·‖₂=2.301e-03\n",
      "base_model.model.transformer.transformer_blocks.2.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.557e-02 max|·|=3.127e-02 ‖·‖₂=4.604e+00\n",
      "base_model.model.transformer.transformer_blocks.2.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=6.636e-06 max|·|=4.689e-05 ‖·‖₂=3.051e-03\n",
      "base_model.model.transformer.transformer_blocks.2.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.102e-02 max|·|=2.211e-02 ‖·‖₂=4.612e+00\n",
      "base_model.model.transformer.transformer_blocks.2.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.522e-06 max|·|=3.985e-05 ‖·‖₂=2.142e-03\n",
      "base_model.model.transformer.transformer_blocks.20.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.564e-02 max|·|=3.126e-02 ‖·‖₂=4.623e+00\n",
      "base_model.model.transformer.transformer_blocks.20.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.112e-06 max|·|=3.030e-05 ‖·‖₂=1.655e-03\n",
      "base_model.model.transformer.transformer_blocks.20.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.557e-02 max|·|=3.126e-02 ‖·‖₂=4.609e+00\n",
      "base_model.model.transformer.transformer_blocks.20.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.734e-06 max|·|=4.181e-05 ‖·‖₂=2.153e-03\n",
      "base_model.model.transformer.transformer_blocks.20.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.557e-02 max|·|=3.126e-02 ‖·‖₂=4.608e+00\n",
      "base_model.model.transformer.transformer_blocks.20.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.000e-06 max|·|=2.723e-05 ‖·‖₂=1.614e-03\n",
      "base_model.model.transformer.transformer_blocks.20.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.565e-02 max|·|=3.126e-02 ‖·‖₂=4.623e+00\n",
      "base_model.model.transformer.transformer_blocks.20.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=7.970e-06 max|·|=3.848e-05 ‖·‖₂=2.482e-03\n",
      "base_model.model.transformer.transformer_blocks.20.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.564e-02 max|·|=3.127e-02 ‖·‖₂=4.620e+00\n",
      "base_model.model.transformer.transformer_blocks.20.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=5.138e-06 max|·|=1.356e-04 ‖·‖₂=2.448e-03\n",
      "base_model.model.transformer.transformer_blocks.20.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.105e-02 max|·|=2.212e-02 ‖·‖₂=4.618e+00\n",
      "base_model.model.transformer.transformer_blocks.20.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.717e-06 max|·|=3.920e-05 ‖·‖₂=2.137e-03\n",
      "base_model.model.transformer.transformer_blocks.21.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.126e-02 ‖·‖₂=4.616e+00\n",
      "base_model.model.transformer.transformer_blocks.21.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.924e-06 max|·|=2.941e-05 ‖·‖₂=1.858e-03\n",
      "base_model.model.transformer.transformer_blocks.21.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.565e-02 max|·|=3.127e-02 ‖·‖₂=4.623e+00\n",
      "base_model.model.transformer.transformer_blocks.21.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=7.208e-06 max|·|=3.827e-05 ‖·‖₂=2.252e-03\n",
      "base_model.model.transformer.transformer_blocks.21.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.563e-02 max|·|=3.127e-02 ‖·‖₂=4.620e+00\n",
      "base_model.model.transformer.transformer_blocks.21.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.369e-06 max|·|=2.884e-05 ‖·‖₂=1.992e-03\n",
      "base_model.model.transformer.transformer_blocks.21.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.127e-02 ‖·‖₂=4.618e+00\n",
      "base_model.model.transformer.transformer_blocks.21.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=7.702e-06 max|·|=3.772e-05 ‖·‖₂=2.398e-03\n",
      "base_model.model.transformer.transformer_blocks.21.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.560e-02 max|·|=3.127e-02 ‖·‖₂=4.617e+00\n",
      "base_model.model.transformer.transformer_blocks.21.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=6.610e-06 max|·|=1.820e-04 ‖·‖₂=3.007e-03\n",
      "base_model.model.transformer.transformer_blocks.21.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.107e-02 max|·|=2.212e-02 ‖·‖₂=4.626e+00\n",
      "base_model.model.transformer.transformer_blocks.21.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.477e-06 max|·|=3.993e-05 ‖·‖₂=2.062e-03\n",
      "base_model.model.transformer.transformer_blocks.3.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.560e-02 max|·|=3.125e-02 ‖·‖₂=4.611e+00\n",
      "base_model.model.transformer.transformer_blocks.3.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=7.659e-07 max|·|=1.599e-05 ‖·‖₂=3.488e-04\n",
      "base_model.model.transformer.transformer_blocks.3.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.562e-02 max|·|=3.126e-02 ‖·‖₂=4.618e+00\n",
      "base_model.model.transformer.transformer_blocks.3.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.187e-06 max|·|=5.256e-05 ‖·‖₂=2.135e-03\n",
      "base_model.model.transformer.transformer_blocks.3.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.558e-02 max|·|=3.125e-02 ‖·‖₂=4.607e+00\n",
      "base_model.model.transformer.transformer_blocks.3.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=8.398e-07 max|·|=2.196e-05 ‖·‖₂=4.244e-04\n",
      "base_model.model.transformer.transformer_blocks.3.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.557e-02 max|·|=3.128e-02 ‖·‖₂=4.606e+00\n",
      "base_model.model.transformer.transformer_blocks.3.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.579e-06 max|·|=5.429e-05 ‖·‖₂=1.926e-03\n",
      "base_model.model.transformer.transformer_blocks.3.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.565e-02 max|·|=3.127e-02 ‖·‖₂=4.622e+00\n",
      "base_model.model.transformer.transformer_blocks.3.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=7.422e-06 max|·|=5.282e-05 ‖·‖₂=3.368e-03\n",
      "base_model.model.transformer.transformer_blocks.3.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.108e-02 max|·|=2.211e-02 ‖·‖₂=4.628e+00\n",
      "base_model.model.transformer.transformer_blocks.3.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.067e-06 max|·|=5.154e-05 ‖·‖₂=2.039e-03\n",
      "base_model.model.transformer.transformer_blocks.4.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.559e-02 max|·|=3.126e-02 ‖·‖₂=4.614e+00\n",
      "base_model.model.transformer.transformer_blocks.4.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.175e-06 max|·|=2.666e-05 ‖·‖₂=1.176e-03\n",
      "base_model.model.transformer.transformer_blocks.4.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.568e-02 max|·|=3.127e-02 ‖·‖₂=4.631e+00\n",
      "base_model.model.transformer.transformer_blocks.4.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.054e-06 max|·|=3.640e-05 ‖·‖₂=2.015e-03\n",
      "base_model.model.transformer.transformer_blocks.4.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.563e-02 max|·|=3.125e-02 ‖·‖₂=4.622e+00\n",
      "base_model.model.transformer.transformer_blocks.4.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=2.969e-06 max|·|=3.303e-05 ‖·‖₂=1.114e-03\n",
      "base_model.model.transformer.transformer_blocks.4.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.565e-02 max|·|=3.128e-02 ‖·‖₂=4.622e+00\n",
      "base_model.model.transformer.transformer_blocks.4.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.162e-06 max|·|=5.204e-05 ‖·‖₂=2.033e-03\n",
      "base_model.model.transformer.transformer_blocks.4.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.563e-02 max|·|=3.127e-02 ‖·‖₂=4.619e+00\n",
      "base_model.model.transformer.transformer_blocks.4.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=7.197e-06 max|·|=3.655e-05 ‖·‖₂=3.122e-03\n",
      "base_model.model.transformer.transformer_blocks.4.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.106e-02 max|·|=2.211e-02 ‖·‖₂=4.622e+00\n",
      "base_model.model.transformer.transformer_blocks.4.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.885e-06 max|·|=4.029e-05 ‖·‖₂=1.947e-03\n",
      "base_model.model.transformer.transformer_blocks.5.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.567e-02 max|·|=3.126e-02 ‖·‖₂=4.629e+00\n",
      "base_model.model.transformer.transformer_blocks.5.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.553e-06 max|·|=2.354e-05 ‖·‖₂=1.204e-03\n",
      "base_model.model.transformer.transformer_blocks.5.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.127e-02 ‖·‖₂=4.621e+00\n",
      "base_model.model.transformer.transformer_blocks.5.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.306e-06 max|·|=4.364e-05 ‖·‖₂=2.040e-03\n",
      "base_model.model.transformer.transformer_blocks.5.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.567e-02 max|·|=3.125e-02 ‖·‖₂=4.630e+00\n",
      "base_model.model.transformer.transformer_blocks.5.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=2.829e-06 max|·|=2.141e-05 ‖·‖₂=1.002e-03\n",
      "base_model.model.transformer.transformer_blocks.5.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.127e-02 ‖·‖₂=4.611e+00\n",
      "base_model.model.transformer.transformer_blocks.5.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.926e-06 max|·|=3.330e-05 ‖·‖₂=2.180e-03\n",
      "base_model.model.transformer.transformer_blocks.5.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.127e-02 ‖·‖₂=4.617e+00\n",
      "base_model.model.transformer.transformer_blocks.5.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=6.910e-06 max|·|=3.348e-05 ‖·‖₂=3.111e-03\n",
      "base_model.model.transformer.transformer_blocks.5.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.105e-02 max|·|=2.212e-02 ‖·‖₂=4.620e+00\n",
      "base_model.model.transformer.transformer_blocks.5.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.691e-06 max|·|=4.451e-05 ‖·‖₂=2.187e-03\n",
      "base_model.model.transformer.transformer_blocks.6.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.559e-02 max|·|=3.126e-02 ‖·‖₂=4.612e+00\n",
      "base_model.model.transformer.transformer_blocks.6.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.325e-06 max|·|=4.138e-05 ‖·‖₂=1.231e-03\n",
      "base_model.model.transformer.transformer_blocks.6.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.564e-02 max|·|=3.127e-02 ‖·‖₂=4.623e+00\n",
      "base_model.model.transformer.transformer_blocks.6.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.926e-06 max|·|=4.364e-05 ‖·‖₂=2.258e-03\n",
      "base_model.model.transformer.transformer_blocks.6.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.558e-02 max|·|=3.126e-02 ‖·‖₂=4.610e+00\n",
      "base_model.model.transformer.transformer_blocks.6.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.820e-06 max|·|=3.488e-05 ‖·‖₂=1.359e-03\n",
      "base_model.model.transformer.transformer_blocks.6.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.126e-02 ‖·‖₂=4.614e+00\n",
      "base_model.model.transformer.transformer_blocks.6.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=8.356e-06 max|·|=6.076e-05 ‖·‖₂=2.796e-03\n",
      "base_model.model.transformer.transformer_blocks.6.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.566e-02 max|·|=3.127e-02 ‖·‖₂=4.628e+00\n",
      "base_model.model.transformer.transformer_blocks.6.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=6.960e-06 max|·|=2.676e-05 ‖·‖₂=3.026e-03\n",
      "base_model.model.transformer.transformer_blocks.6.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.107e-02 max|·|=2.211e-02 ‖·‖₂=4.627e+00\n",
      "base_model.model.transformer.transformer_blocks.6.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.350e-06 max|·|=4.368e-05 ‖·‖₂=2.033e-03\n",
      "base_model.model.transformer.transformer_blocks.7.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.563e-02 max|·|=3.125e-02 ‖·‖₂=4.618e+00\n",
      "base_model.model.transformer.transformer_blocks.7.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.070e-06 max|·|=2.993e-05 ‖·‖₂=1.079e-03\n",
      "base_model.model.transformer.transformer_blocks.7.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.567e-02 max|·|=3.126e-02 ‖·‖₂=4.630e+00\n",
      "base_model.model.transformer.transformer_blocks.7.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.889e-06 max|·|=3.380e-05 ‖·‖₂=1.922e-03\n",
      "base_model.model.transformer.transformer_blocks.7.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.560e-02 max|·|=3.126e-02 ‖·‖₂=4.612e+00\n",
      "base_model.model.transformer.transformer_blocks.7.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=2.996e-06 max|·|=2.771e-05 ‖·‖₂=1.065e-03\n",
      "base_model.model.transformer.transformer_blocks.7.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.563e-02 max|·|=3.127e-02 ‖·‖₂=4.619e+00\n",
      "base_model.model.transformer.transformer_blocks.7.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.544e-06 max|·|=2.675e-05 ‖·‖₂=2.040e-03\n",
      "base_model.model.transformer.transformer_blocks.7.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.565e-02 max|·|=3.127e-02 ‖·‖₂=4.624e+00\n",
      "base_model.model.transformer.transformer_blocks.7.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=7.403e-06 max|·|=5.843e-05 ‖·‖₂=3.359e-03\n",
      "base_model.model.transformer.transformer_blocks.7.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.107e-02 max|·|=2.212e-02 ‖·‖₂=4.628e+00\n",
      "base_model.model.transformer.transformer_blocks.7.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.477e-06 max|·|=4.777e-05 ‖·‖₂=2.111e-03\n",
      "base_model.model.transformer.transformer_blocks.8.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.556e-02 max|·|=3.126e-02 ‖·‖₂=4.603e+00\n",
      "base_model.model.transformer.transformer_blocks.8.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.096e-06 max|·|=2.884e-05 ‖·‖₂=1.379e-03\n",
      "base_model.model.transformer.transformer_blocks.8.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.562e-02 max|·|=3.127e-02 ‖·‖₂=4.615e+00\n",
      "base_model.model.transformer.transformer_blocks.8.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.991e-06 max|·|=4.133e-05 ‖·‖₂=2.227e-03\n",
      "base_model.model.transformer.transformer_blocks.8.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.559e-02 max|·|=3.126e-02 ‖·‖₂=4.610e+00\n",
      "base_model.model.transformer.transformer_blocks.8.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=4.257e-06 max|·|=2.952e-05 ‖·‖₂=1.451e-03\n",
      "base_model.model.transformer.transformer_blocks.8.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.561e-02 max|·|=3.127e-02 ‖·‖₂=4.615e+00\n",
      "base_model.model.transformer.transformer_blocks.8.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=8.151e-06 max|·|=3.378e-05 ‖·‖₂=2.480e-03\n",
      "base_model.model.transformer.transformer_blocks.8.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.562e-02 max|·|=3.127e-02 ‖·‖₂=4.620e+00\n",
      "base_model.model.transformer.transformer_blocks.8.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=7.442e-06 max|·|=5.063e-05 ‖·‖₂=3.404e-03\n",
      "base_model.model.transformer.transformer_blocks.8.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.104e-02 max|·|=2.211e-02 ‖·‖₂=4.613e+00\n",
      "base_model.model.transformer.transformer_blocks.8.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=7.301e-06 max|·|=4.297e-05 ‖·‖₂=2.345e-03\n",
      "base_model.model.transformer.transformer_blocks.9.attn.to_k.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.564e-02 max|·|=3.126e-02 ‖·‖₂=4.623e+00\n",
      "base_model.model.transformer.transformer_blocks.9.attn.to_k.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.662e-06 max|·|=4.111e-05 ‖·‖₂=1.322e-03\n",
      "base_model.model.transformer.transformer_blocks.9.attn.to_out.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.563e-02 max|·|=3.127e-02 ‖·‖₂=4.620e+00\n",
      "base_model.model.transformer.transformer_blocks.9.attn.to_out.0.lora_B.default.weight  shape=(1024, 64)  mean|·|=6.740e-06 max|·|=3.125e-05 ‖·‖₂=2.123e-03\n",
      "base_model.model.transformer.transformer_blocks.9.attn.to_q.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.559e-02 max|·|=3.125e-02 ‖·‖₂=4.614e+00\n",
      "base_model.model.transformer.transformer_blocks.9.attn.to_q.lora_B.default.weight  shape=(1024, 64)  mean|·|=3.634e-06 max|·|=3.453e-05 ‖·‖₂=1.298e-03\n",
      "base_model.model.transformer.transformer_blocks.9.attn.to_v.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.569e-02 max|·|=3.127e-02 ‖·‖₂=4.631e+00\n",
      "base_model.model.transformer.transformer_blocks.9.attn.to_v.lora_B.default.weight  shape=(1024, 64)  mean|·|=8.654e-06 max|·|=3.162e-05 ‖·‖₂=2.565e-03\n",
      "base_model.model.transformer.transformer_blocks.9.ff.ff.0.0.lora_A.default.weight  shape=(64, 1024)  mean|·|=1.560e-02 max|·|=3.127e-02 ‖·‖₂=4.612e+00\n",
      "base_model.model.transformer.transformer_blocks.9.ff.ff.0.0.lora_B.default.weight  shape=(2048, 64)  mean|·|=6.408e-06 max|·|=4.860e-05 ‖·‖₂=3.030e-03\n",
      "base_model.model.transformer.transformer_blocks.9.ff.ff.2.lora_A.default.weight  shape=(64, 2048)  mean|·|=1.105e-02 max|·|=2.211e-02 ‖·‖₂=4.620e+00\n",
      "base_model.model.transformer.transformer_blocks.9.ff.ff.2.lora_B.default.weight  shape=(1024, 64)  mean|·|=5.873e-06 max|·|=4.085e-05 ‖·‖₂=1.905e-03\n",
      "\n",
      "Summary:\n",
      "  Total LoRA parameters : 20,971,520\n",
      "  Combined Frobenius ‖Δ‖₂ : 5.464e+01\n",
      "  Avg per-param scale    : 1.193e-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from f5_tac.model.utils import list_str_to_idx\n",
    "\n",
    "# 2) Extract embedding weights\n",
    "#    adjust the key if your checkpoint nests it differently\n",
    "Wb = sd_before[\"base_model.model.transformer.text_embed.text_embed.weight\"]\n",
    "Wa = sd_after[\"base_model.model.transformer.text_embed.text_embed.weight\"]\n",
    "\n",
    "ib = sd_before[\"base_model.model.transformer.input_embed.proj.weight\"]\n",
    "ia = sd_after[\"base_model.model.transformer.input_embed.proj.weight\"]\n",
    "\n",
    "# 3) Compute absolute difference\n",
    "delta = (Wa - Wb).abs()\n",
    "\n",
    "idelta = (ib - ia).abs()\n",
    "\n",
    "spk_chg_token = \"<utt>\"\n",
    "\n",
    "# 4) Metrics\n",
    "speaker_chg_token_idx = list_str_to_idx([spk_chg_token], vocab_char_map=vocab_char_map)\n",
    "\n",
    "max_overall   = delta.max().item()\n",
    "max_speaker   = delta[speaker_chg_token_idx].max().item()\n",
    "\n",
    "print(\"\\n== Text Embedding Drift ==\")\n",
    "print(f\"→ Max text embedding matrix change |Δ| overall:                  {max_overall:.6f}\")\n",
    "print(f\"→ Max |Δ| on token {speaker_chg_token_idx}: {max_speaker:.6f}\")\n",
    "\n",
    "print(\"\\n== Input Embedding Drift ==\")\n",
    "print(f\"→ Mean input embedding weight change |Δ| overall:                  {idelta.max().item():.6f}\")\n",
    "\n",
    "print(\"== LoRA Weight Scales ==\")\n",
    "inspect_lora_weights(sd_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d250744f-51a0-4ece-af35-1c76f15eeda1",
   "metadata": {},
   "source": [
    "# Check Text Embedding Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae6864b-b781-493e-a58c-2c44afd2f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from importlib.resources import files\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from cached_path import cached_path\n",
    "\n",
    "import yaml\n",
    "\n",
    "# --- MODIFICATION: Import your new modules ---\n",
    "from f5_tac.model.cfm import CFMWithTAC\n",
    "from f5_tac.model.reccfm import CFMWithTACRecon\n",
    "from f5_tac.model.backbones.dittac import DiTWithTAC\n",
    "from f5_tac.model.trainer import Trainer\n",
    "from f5_tac.configs.model_kwargs import lora_configv2\n",
    "from f5_tts.model.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "903c7716-4389-4621-aeb1-dc0d90479173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained model from: /work/users/r/p/rphadke/JSALT/ckpts/pretrained_model_1250000.safetensors\n",
      "Loaded tokenizer with vocab size: 2546\n"
     ]
    }
   ],
   "source": [
    "pretrain = \"/work/users/r/p/rphadke/JSALT/ckpts/pretrained_model_1250000.safetensors\"\n",
    "local_pretrain_path = pretrain\n",
    "print(f\"Using pretrained model from: {local_pretrain_path}\")\n",
    "\n",
    "# --- 2. Setup Tokenizer ---\n",
    "tokenizer_path = \"/work/users/r/p/rphadke/JSALT/fisher_chunks_0.1K_v2.1/vocab.txt\"\n",
    "vocab_char_map, vocab_size = get_tokenizer(tokenizer_path,\"custom\")\n",
    "print(f\"Loaded tokenizer with vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92698782-ea85-497b-8622-98c0bf372382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating F5-TAC models...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Define Model Architecture and Mel Spectrogram settings ---\n",
    "# These should match the architecture of the pretrained model you are loading.\n",
    "# mel_spec_kwargs = dict(\n",
    "#     n_fft=1024, hop_length=256, win_length=1024,\n",
    "#     n_mel_channels=100, target_sample_rate=24000, mel_spec_type=\"vocos\",\n",
    "# )\n",
    "\n",
    "\n",
    "# dit_cfg = dict(\n",
    "#     dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4\n",
    "# )\n",
    "\n",
    "from f5_tac.configs.model_kwargs import mel_spec_kwargs, dit_cfg\n",
    "\n",
    "# --- 4. Instantiate Your New Models ---\n",
    "print(\"Instantiating F5-TAC models...\")\n",
    "transformer_backbone = DiTWithTAC(\n",
    "    **dit_cfg,\n",
    "    num_speakers=2, # Critical for TAC blocks\n",
    "    text_num_embeds=vocab_size,\n",
    "    mel_dim=mel_spec_kwargs[\"n_mel_channels\"]\n",
    ")\n",
    "\n",
    "model = CFMWithTACRecon(\n",
    "    transformer=transformer_backbone,\n",
    "    mel_spec_kwargs=mel_spec_kwargs,\n",
    "    vocab_char_map=vocab_char_map,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdeb253a-f1ec-43db-ba22-ffa4bf2e748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4294,  1.1332, -0.3012,  ...,  0.7869, -0.0200, -0.0887],\n",
       "        [ 0.3353,  0.2027,  1.1162,  ...,  0.2878,  0.3035,  0.0241],\n",
       "        [-0.1944, -0.5891, -0.3902,  ..., -0.8912, -0.2729,  1.2370],\n",
       "        ...,\n",
       "        [-0.9431,  0.1731, -0.1253,  ...,  0.5341, -1.6237, -0.4977],\n",
       "        [ 0.3542,  0.4017,  0.2221,  ..., -0.7915,  0.5031,  0.7767],\n",
       "        [ 0.4798, -0.1583,  0.2985,  ..., -0.0142, -1.1777, -0.1104]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading pretrained weights...\")\n",
    "# 1) Load the raw checkpoint\n",
    "if local_pretrain_path.endswith(\".safetensors\"):\n",
    "    from safetensors.torch import load_file\n",
    "    ckpt = load_file(local_pretrain_path, device=\"cpu\")\n",
    "else:\n",
    "    ckpt = torch.load(local_pretrain_path, map_location=\"cpu\")\n",
    "\n",
    "# print(ckpt)\n",
    "\n",
    "# 2) Unwrap any nesting\n",
    "state = (\n",
    "    ckpt.get(\"model_state_dict\", \n",
    "    ckpt.get(\"ema_model_state_dict\", ckpt))\n",
    ")\n",
    "\n",
    "# 3) Strip an ‘ema_model.’ prefix if it sneaked in\n",
    "state = {k.replace(\"ema_model.\", \"\"): v for k, v in state.items()}\n",
    "\n",
    "old_embed_weight = state[\"transformer.text_embed.text_embed.weight\"]\n",
    "old_embed_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "685e8a2e-572c-42da-a003-77e650328d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2546, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_embed_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8df54ab-60ca-4d07-81e0-cde4e0f12c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.2939e-01,  1.1332e+00, -3.0124e-01,  ...,  7.8693e-01,\n",
       "         -1.9993e-02, -8.8680e-02],\n",
       "        [ 3.3533e-01,  2.0267e-01,  1.1162e+00,  ...,  2.8777e-01,\n",
       "          3.0348e-01,  2.4130e-02],\n",
       "        [-1.9440e-01, -5.8913e-01, -3.9018e-01,  ..., -8.9120e-01,\n",
       "         -2.7291e-01,  1.2370e+00],\n",
       "        ...,\n",
       "        [ 3.5422e-01,  4.0167e-01,  2.2213e-01,  ..., -7.9148e-01,\n",
       "          5.0305e-01,  7.7668e-01],\n",
       "        [ 4.7975e-01, -1.5830e-01,  2.9851e-01,  ..., -1.4222e-02,\n",
       "         -1.1777e+00, -1.1040e-01],\n",
       "        [ 2.6985e-04, -1.6784e-02,  2.9970e-03,  ...,  1.5645e-03,\n",
       "          6.0575e-03,  1.3508e-02]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    new_embed_weights = torch.cat([\n",
    "        old_embed_weight,                               # copy existing rows\n",
    "        torch.randn(1, old_embed_weight.shape[1]) * 0.01  # random init new row\n",
    "    ], dim=0)\n",
    "new_embed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0ca178c-bd09-4957-b4ad-7cb5504607e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2547, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f5f4b2-8b03-4b15-aa8d-2a71fa3c91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the weight in the state dict\n",
    "state[\"transformer.text_embed.text_embed.weight\"] = new_embed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71e92358-2820-4bca-8001-f40a098a849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ loaded partial state:\n",
      "  • missing   (should only be tac module keys)   : ['transformer.transformer_blocks.0.tac.alpha', 'transformer.transformer_blocks.0.tac.transform_shared.0.weight', 'transformer.transformer_blocks.0.tac.transform_shared.0.bias', 'transformer.transformer_blocks.0.tac.transform_avg.0.weight', 'transformer.transformer_blocks.0.tac.transform_avg.0.bias'] …\n",
      "  • unexpected  : ['initted', 'step'] …\n"
     ]
    }
   ],
   "source": [
    "# 5) Finally load with strict=False to pick up whatever lines up\n",
    "incompatible = model.load_state_dict(state, strict=False)\n",
    "\n",
    "\n",
    "print(\"✔ loaded partial state:\")\n",
    "print(\"  • missing   (should only be tac module keys)   :\", incompatible.missing_keys[:5], \"…\")\n",
    "print(\"  • unexpected  :\", incompatible.unexpected_keys[:5], \"…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e03b9646-bfe4-401d-8021-3c662c441f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,971,520 || all params: 773,507,706 || trainable%: 2.7112\n",
      "base_model.model.transformer.text_embed.text_embed.weight\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, get_peft_model\n",
    "model = get_peft_model(model, lora_configv2)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"tac\" in name or \"text_embed.text_embed\" in name or \"3.dwconv\" in name:\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    if \"text_embed.text_embed\" in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18dcf9e5-3be3-49a1-9ec9-1b4f9cfcf7fa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): PeftModel(\n",
      "      (base_model): LoraModel(\n",
      "        (model): CFMWithTACRecon(\n",
      "          (mel_spec): MelSpec()\n",
      "          (transformer): DiTWithTAC(\n",
      "            (time_embed): TimestepEmbedding(\n",
      "              (time_embed): SinusPositionEmbedding()\n",
      "              (time_mlp): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                (1): SiLU()\n",
      "                (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (text_embed): TextEmbedding(\n",
      "              (text_embed): Embedding(2547, 512)\n",
      "              (text_blocks): Sequential(\n",
      "                (0): ConvNeXtV2Block(\n",
      "                  (dwconv): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), groups=512)\n",
      "                  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "                  (pwconv1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (grn): GRN()\n",
      "                  (pwconv2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=512, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "                (1): ConvNeXtV2Block(\n",
      "                  (dwconv): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), groups=512)\n",
      "                  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "                  (pwconv1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (grn): GRN()\n",
      "                  (pwconv2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=512, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "                (2): ConvNeXtV2Block(\n",
      "                  (dwconv): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), groups=512)\n",
      "                  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "                  (pwconv1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (grn): GRN()\n",
      "                  (pwconv2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=512, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "                (3): ConvNeXtV2Block(\n",
      "                  (dwconv): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), groups=512)\n",
      "                  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "                  (pwconv1): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (grn): GRN()\n",
      "                  (pwconv2): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=512, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (input_embed): InputEmbedding(\n",
      "              (proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=712, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=712, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (conv_pos_embed): ConvPositionEmbedding(\n",
      "                (conv1d): Sequential(\n",
      "                  (0): lora.Conv1d(\n",
      "                    (base_layer): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=16)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Conv1d(1024, 64, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Conv1d(64, 1024, kernel_size=(1,), stride=(1,), groups=16, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (1): Mish()\n",
      "                  (2): lora.Conv1d(\n",
      "                    (base_layer): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=16)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Conv1d(1024, 64, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Conv1d(64, 1024, kernel_size=(1,), stride=(1,), groups=16, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (3): Mish()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (rotary_embed): RotaryEmbedding()\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0-21): 22 x DiTBlockWithTAC(\n",
      "                (attn_norm): AdaLayerNorm(\n",
      "                  (silu): SiLU()\n",
      "                  (linear): Linear(in_features=1024, out_features=6144, bias=True)\n",
      "                  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
      "                )\n",
      "                (attn): Attention(\n",
      "                  (to_q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (to_k): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (to_v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.1, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                    (1): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (ff_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
      "                (ff): FeedForward(\n",
      "                  (ff): Sequential(\n",
      "                    (0): Sequential(\n",
      "                      (0): lora.Linear(\n",
      "                        (base_layer): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "                        (lora_dropout): ModuleDict(\n",
      "                          (default): Dropout(p=0.1, inplace=False)\n",
      "                        )\n",
      "                        (lora_A): ModuleDict(\n",
      "                          (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                        )\n",
      "                        (lora_B): ModuleDict(\n",
      "                          (default): Linear(in_features=64, out_features=2048, bias=False)\n",
      "                        )\n",
      "                        (lora_embedding_A): ParameterDict()\n",
      "                        (lora_embedding_B): ParameterDict()\n",
      "                        (lora_magnitude_vector): ModuleDict()\n",
      "                      )\n",
      "                      (1): GELU(approximate='tanh')\n",
      "                    )\n",
      "                    (1): Dropout(p=0.1, inplace=False)\n",
      "                    (2): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.1, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=2048, out_features=64, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (tac): TAC(\n",
      "                  (transform_shared): Sequential(\n",
      "                    (0): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                    (1): ReLU()\n",
      "                    (2): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (transform_avg): Sequential(\n",
      "                    (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "                    (1): ReLU()\n",
      "                    (2): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (transform_final): Sequential(\n",
      "                    (0): Linear(in_features=6144, out_features=1024, bias=True)\n",
      "                    (1): ReLU()\n",
      "                    (2): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (norm_out): AdaLayerNorm_Final(\n",
      "              (silu): SiLU()\n",
      "              (linear): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "              (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
      "            )\n",
      "            (proj_out): Linear(in_features=1024, out_features=100, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5719c5e-a3b0-4a81-b8c8-0f8254574ff3",
   "metadata": {},
   "source": [
    "# Text to Index New Function Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ba5c04-5773-4e39-86e6-6bd6429ccf93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<utt>\n"
     ]
    }
   ],
   "source": [
    "for key in vocab_char_map.keys():\n",
    "    if \"utt\" in key:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a76016e9-3c6c-43fe-b193-462c31b8fbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  62,  765,  250,    0,  507,    0,  369,  325,  765,  325,  940,   62,\n",
       "          615,  615, 1236,   -1,   -1,    0,  834,  940,  325,  337,  325,  940,\n",
       "            0, 2545,    0,    0,  325,   62, 1082,  507,  765,  369,    0,   62,\n",
       "         1082,    0,  441,  827,  704,  325,    0, 2545,    0,    0,  441,  325,\n",
       "          615,  615,  827,    0,   62,  765,  250, 1236,    0, 2545,    0,    0,\n",
       "          441,  827, 1149,    0,   62,  940,  325,    0, 1236,  827, 1147,    0,\n",
       "         2545,    0,    0,  369,  827,  827,  250,    0, 2545,    0,    0,  250,\n",
       "          827,    0, 1236,  827, 1147,    0,  441,   62, 1148,  325,    0,   62,\n",
       "          765, 1236,    0,  507,  250,  325,   62,    0, 1149,  441,   62, 1082,\n",
       "            7,  973,    0,  369,  827,  507,  765,  369,    0,  827,  765,    0,\n",
       "         2545,    0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def list_str_to_idx(\n",
    "    text: list[str] | list[list[str]],\n",
    "    vocab_char_map: dict[str, int],  # {char: idx}\n",
    "    padding_value=-1,\n",
    "):\n",
    "    list_idx_tensors = []\n",
    "    for t in text:\n",
    "        idxs = []\n",
    "        i = 0\n",
    "        while i < len(t):\n",
    "            if t[i : i + 5] == \"<utt>\":  # Detect \"<utt>\"\n",
    "                idxs.append(vocab_char_map.get(\"<utt>\", 0))\n",
    "                i += 5  # Skip over \"<utt>\"\n",
    "            elif t[i : i + 5] == \"<sil>\":  # Detect \"<utt>\"\n",
    "                idxs.append(-1)\n",
    "                i += 5  # Skip over \"<utt>\"\n",
    "            else:\n",
    "                idxs.append(vocab_char_map.get(t[i], 0))\n",
    "                i += 1\n",
    "        list_idx_tensors.append(torch.tensor(idxs))\n",
    "    text = pad_sequence(list_idx_tensors, padding_value=padding_value, batch_first=True)\n",
    "    return text\n",
    "\n",
    "text = \"and i generally<sil><sil> prefer <utt>  eating at home <utt>  hello andy <utt>  how are you <utt>  good <utt>  do you have any idea what's going on <utt> \"\n",
    "list_str_to_idx([text], vocab_char_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b3b25cf-ab03-4cbb-bd15-2b975b5dc4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from f5_tac.model.utils import list_str_to_idx\n",
    "\n",
    "# from f5_tts.model.utils import list_str_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8461525a-ea55-415d-8250-f4e1577bcd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loading conversation dataset...\n",
      "[\"e- e- everything could (( )) to something i think even gossiping <utt> they're (( enjoyable )) <utt> yeah but anything excessive i think ah <utt> e- it's something <utt> that <utt> challenges us <utt> at cer- certain points to rethink <utt> what <utt>\"]\n",
      "tensor([[ 325,   13,    0,  325,   13,    0,  325, 1148,  325,  940, 1236, 1082,\n",
      "          441,  507,  765,  369,    0,  143,  827, 1147,  615,  250,    0,    8,\n",
      "            8,    0,    9,    9,    0, 1082,  827,    0,  973,  827,  704,  325,\n",
      "         1082,  441,  507,  765,  369,    0,  507,    0, 1082,  441,  507,  765,\n",
      "          560,    0,  325, 1148,  325,  765,    0,  369,  827,  973,  973,  507,\n",
      "          834,  507,  765,  369,    0, 2545,    0, 1082,  441,  325, 1236,    7,\n",
      "          940,  325,    0,    8,    8,    0,  325,  765,  508,  827, 1236,   62,\n",
      "           78,  615,  325,    0,    9,    9,    0, 2545,    0, 1236,  325,   62,\n",
      "          441,    0,   78, 1147, 1082,    0,   62,  765, 1236, 1082,  441,  507,\n",
      "          765,  369,    0,  325, 1184,  143,  325,  973,  973,  507, 1148,  325,\n",
      "            0,  507,    0, 1082,  441,  507,  765,  560,    0,   62,  441,    0,\n",
      "         2545,    0,  325,   13,    0,  507, 1082,    7,  973,    0,  973,  827,\n",
      "          704,  325, 1082,  441,  507,  765,  369,    0, 2545,    0, 1082,  441,\n",
      "           62, 1082,    0, 2545,    0,  143,  441,   62,  615,  615,  325,  765,\n",
      "          369,  325,  973,    0, 1147,  973,    0, 2545,    0,   62, 1082,    0,\n",
      "          143,  325,  940,   13,    0,  143,  325,  940, 1082,   62,  507,  765,\n",
      "            0,  834,  827,  507,  765, 1082,  973,    0, 1082,  827,    0,  940,\n",
      "          325, 1082,  441,  507,  765,  560,    0, 2545,    0, 1149,  441,   62,\n",
      "         1082,    0, 2545]])\n"
     ]
    }
   ],
   "source": [
    "from f5_tac.model.dataset import load_conversation_dataset, conversation_collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_path = \"/work/users/r/p/rphadke/JSALT/fisher_chunks_0.1K_v2.1\"\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "train_dataset = load_conversation_dataset(\n",
    "    dataset_path=dataset_path,\n",
    "    mel_spec_kwargs=mel_spec_kwargs\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=conversation_collate_fn,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    batch_size=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch[\"text_A\"])\n",
    "    print(list_str_to_idx(batch[\"text_A\"], vocab_char_map))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8d88fb4-3aa5-434b-818a-488baa3d325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loading conversation dataset...\n",
      "[\"yeah i think for me it would be thanksgiving um i'm jewish so i don't have the christmas thing to compare with <utt> and um <utt> [mn] so it's really i i do that and i do um passover with my family but that is more <utt> (( uh )) <utt> can get boring sometimes [laughter] and thank- i just really like to uh <utt> [mn] get together with my family so i like that <utt>\"]\n",
      "tensor([[1236,  325,   62,  441,    0,  507,    0, 1082,  441,  507,  765,  560,\n",
      "            0,  337,  827,  940,    0,  704,  325,    0,  507, 1082,    0, 1149,\n",
      "          827, 1147,  615,  250,    0,   78,  325,    0, 1082,  441,   62,  765,\n",
      "          560,  973,  369,  507, 1148,  507,  765,  369,    0, 1147,  704,    0,\n",
      "          507,    7,  704,    0,  508,  325, 1149,  507,  973,  441,    0,  973,\n",
      "          827,    0,  507,    0,  250,  827,  765,    7, 1082,    0,  441,   62,\n",
      "         1148,  325,    0, 1082,  441,  325,    0,  143,  441,  940,  507,  973,\n",
      "         1082,  704,   62,  973,    0, 1082,  441,  507,  765,  369,    0, 1082,\n",
      "          827,    0,  143,  827,  704,  834,   62,  940,  325,    0, 1149,  507,\n",
      "         1082,  441,    0,    0, 1147, 1082, 1082,   29,    0,   62,  765,  250,\n",
      "            0, 1147,  704,    0,    0, 1147, 1082, 1082,   29,    0,   58,  704,\n",
      "          765,   60,    0,  973,  827,    0,  507, 1082,    7,  973,    0,  940,\n",
      "          325,   62,  615,  615, 1236,    0,  507,    0,  507,    0,  250,  827,\n",
      "            0, 1082,  441,   62, 1082,    0,   62,  765,  250,    0,  507,    0,\n",
      "          250,  827,    0, 1147,  704,    0,  834,   62,  973,  973,  827, 1148,\n",
      "          325,  940,    0, 1149,  507, 1082,  441,    0,  704, 1236,    0,  337,\n",
      "           62,  704,  507,  615, 1236,    0,   78, 1147, 1082,    0, 1082,  441,\n",
      "           62, 1082,    0,  507,  973,    0,  704,  827,  940,  325,    0,    0,\n",
      "         1147, 1082, 1082,   29,    0,    8,    8,    0, 1147,  441,    0,    9,\n",
      "            9,    0,    0, 1147, 1082, 1082,   29,    0,  143,   62,  765,    0,\n",
      "          369,  325, 1082,    0,   78,  827,  940,  507,  765,  369,    0,  973,\n",
      "          827,  704,  325, 1082,  507,  704,  325,  973,    0,   58,  615,   62,\n",
      "         1147,  369,  441, 1082,  325,  940,   60,    0,   62,  765,  250,    0,\n",
      "         1082,  441,   62,  765,  560,   13,    0,  507,    0,  508, 1147,  973,\n",
      "         1082,    0,  940,  325,   62,  615,  615, 1236,    0,  615,  507,  560,\n",
      "          325,    0, 1082,  827,    0, 1147,  441,    0,    0, 1147, 1082, 1082,\n",
      "           29,    0,   58,  704,  765,   60,    0,  369,  325, 1082,    0, 1082,\n",
      "          827,  369,  325, 1082,  441,  325,  940,    0, 1149,  507, 1082,  441,\n",
      "            0,  704, 1236,    0,  337,   62,  704,  507,  615, 1236,    0,  973,\n",
      "          827,    0,  507,    0,  615,  507,  560,  325,    0, 1082,  441,   62,\n",
      "         1082,    0,    0, 1147, 1082, 1082,   29]])\n"
     ]
    }
   ],
   "source": [
    "from f5_tac.model.dataset import load_conversation_dataset, conversation_collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_path = \"/work/users/r/p/rphadke/JSALT/fisher_chunks_0.1K_v2.1\"\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "train_dataset = load_conversation_dataset(\n",
    "    dataset_path=dataset_path,\n",
    "    mel_spec_kwargs=mel_spec_kwargs\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=conversation_collate_fn,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    batch_size=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch[\"text_A\"])\n",
    "    print(list_str_to_idx(batch[\"text_A\"], vocab_char_map))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
